{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP7404 Computational Intelligence and Machine Learnin <br> Group Project: Boosted Random Foreset\n",
    " \n",
    "Team Member:\n",
    "```\n",
    "Lee Kai Shing 3035562540\n",
    "Mak Tak Hei 3035420273\n",
    "Lui Sin Ying Bianca 2010085577\n",
    "Lo Rocky 3035420077\n",
    "Chan Wing Hei 3035186243\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "As suggested as the name of the algorithm, Boosted Random Forest (BRF) takes the advantages from Boosting and Bagging (from Random Forest) which aims at providing a better performance with a smaller forest, while avoiding overfitting at the same time.\n",
    "\n",
    "Our study is based on the research paper titled as <a href=\"http://www.vision.cs.chubu.ac.jp/MPRG/C_group/C058_mishina2014.pdf\">\"Boosted Random Forest\"</a> (Mishina, Tsuchiya & Fujiyoshi, 2014). From this paper, a very illustrative figure is found, which gives a graphical description of BRF algorithm.\n",
    "<img src=\"Pics/brf.png\" width=\"400\">\n",
    "\n",
    "This notebook serves as a demonstration of the group project from class COMP7404 Computational Intelligence and Machine Learning. For the purpose of the project, <b><u>a python module on BRF is implemented</u></b>, which is also used for experiments and demonstrations below. In the following sections, codes developed as an implementation of BRF will be demonstrated, along with some <b><u>experiment results which demonstrate how BRF improves RF</u></b>. At the end of this notebook, an modification to the original BRF algorithm is going to be preseneted, which makes a <b><u> great improvement on training time, without sacrifice on accuracy</u><b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Settings\n",
    "***\n",
    "\n",
    "This section explains the experiments procedures taken to obtain the results shown in this notebook.\n",
    "\n",
    "First, lets prepare this notebook by importing the required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Some packages we will use in this notebook \"\"\"\n",
    "import numpy as np                          # For numerical operations\n",
    "import pandas as pd                         # For handling data\n",
    "from time import time                       # For evaluating training times\n",
    "from sklearn.metrics import accuracy_score  # For evalutaing prediction accuracies\n",
    "import matplotlib.pyplot as plt             # For plotting experiment results\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Procedures:\n",
    "- Split train/test data\n",
    "- For each setting (#trees, max_dpeth):\n",
    "    - Repeat 20 times:\n",
    "        - Train a new BRF classifier\n",
    "        - Give prediction \n",
    "        - Measure mean accuracy\n",
    "\n",
    "### Dataset: \"Spamebase\"\n",
    "- Determine if a message is spam based by a bag-of-words dataset\n",
    "- % of spams:\n",
    "  - Training set: ~40%\n",
    "  - Testing set: ~36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_people</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_addresses</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_business</th>\n",
       "      <th>word_freq_email</th>\n",
       "      <th>word_freq_you</th>\n",
       "      <th>word_freq_credit</th>\n",
       "      <th>word_freq_your</th>\n",
       "      <th>word_freq_font</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>word_freq_money</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_hpl</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_650</th>\n",
       "      <th>word_freq_lab</th>\n",
       "      <th>word_freq_labs</th>\n",
       "      <th>word_freq_telnet</th>\n",
       "      <th>word_freq_857</th>\n",
       "      <th>word_freq_data</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_85</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>word_freq_1999</th>\n",
       "      <th>word_freq_parts</th>\n",
       "      <th>word_freq_pm</th>\n",
       "      <th>word_freq_direct</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_meeting</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_project</th>\n",
       "      <th>word_freq_re</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_table</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  ...   capital_run_length_total  Spam\n",
       "0            0.00               0.64  ...                        278     1\n",
       "1            0.21               0.28  ...                       1028     1\n",
       "2            0.06               0.00  ...                       2259     1\n",
       "3            0.00               0.00  ...                        191     1\n",
       "4            0.00               0.00  ...                        191     1\n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" Read dataset to be used \"\"\"\n",
    "# In this example, a dataset about spamming is used\n",
    "data = pd.read_csv('spambase.csv',sep=\",\")\n",
    "display(data.head())\n",
    "\n",
    "\"\"\" Some preprocessing on data \"\"\"\n",
    "# Number of features\n",
    "m = data.shape[1]\n",
    "# Remove unwanted features\n",
    "X = data.iloc[:,0:48]\n",
    "y = data.iloc[:,(m-1):]\n",
    "\n",
    "# Turn data into onehot format\n",
    "X_onehot = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of spams (train):  Spam    0.394045\n",
      "dtype: float64\n",
      "% of spams (train):  Spam    0.402899\n",
      "dtype: float64\n",
      "% of spams (test):  Spam    0.367507\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\" Splitting training and testing data \"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_onehot, y, test_size=0.25, random_state=33)\n",
    "\n",
    "\"\"\" Some information about the dataset \"\"\"\n",
    "print(\"% of spams (train): \", np.mean(y))\n",
    "print(\"% of spams (train): \", np.mean(y_train))\n",
    "print(\"% of spams (test): \", np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Python Module - BoostedRandomForest\n",
    "---\n",
    "During our studies on BRF, a python module is implemented, which is developed in a style similar to that from sklearn. So it is easy to adopt and easy to use.\n",
    "\n",
    "The following cells show how a sample BRF classifier is created, and how to train and predict with our BRF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BRF classifier module\n",
    "from BoostedRandomForest import BoostedRandomForest\n",
    "\n",
    "# Create a BRF classifier\n",
    "my_first_brf = BoostedRandomForest(T=150, sample_portion=0.6, \n",
    "                                   depth_max=20, criterion='entropy', \n",
    "                                   eps_ub=1, eps_lb=0, \n",
    "                                   eps_exceed_limit=5, early_stop=False, \n",
    "                                   weight_update=True, boosting=True, \n",
    "                                   debug_msg=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  0.4607100486755371\n",
      "Acc:  0.8175499565595135\n"
     ]
    }
   ],
   "source": [
    "# Train our BRF classifier, just like sklearn!\n",
    "start = time()\n",
    "my_first_brf.fit(X_train, y_train)\n",
    "end = time()\n",
    "print(\"Training Time: \", end-start)\n",
    "\n",
    "# Give prediction, easy\n",
    "pred = my_first_brf.ensemble_predict(X_test)\n",
    "\n",
    "# Calcuate Accuracy\n",
    "print(\"Acc: \", accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results\n",
    "---\n",
    "With our BRF python package prepared, experiments now become easy. As mentioned in the previous section, the accuracies are obtained by taking an average from 20 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training classifier #0\n",
      "Time from start:  63.904162883758545\n",
      "Now training classifier #1\n",
      "Time from start:  123.62151098251343\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sample Procedures of Experiment \"\"\"\n",
    "# Parameters for BRF classifier\n",
    "test_brf_params = {'T': 250,\n",
    "              'depth_max': 20,\n",
    "              'weight_update': True,\n",
    "              'boosting': True,\n",
    "             }\n",
    "\n",
    "# List of accuracies in each trial\n",
    "test_brf_accs = []\n",
    "\n",
    "# Loop for 20 trials\n",
    "start = time()\n",
    "for i in range(20) :\n",
    "    print(\"Now training classifier #{}\".format(i))\n",
    "    # Create a new BRF classifier\n",
    "    test_brf_clf = BoostedRandomForest(**test_brf_params)\n",
    "    # Train BRF classifier\n",
    "    test_brf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtain accuracies from BRF classifier object\n",
    "    test_brf_accs.append(test_brf_clf.train_accs)\n",
    "    \n",
    "    print(\"Time from start: \", time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max acc:, 0.971159420289855, Mean acc: 0.9567159420289856\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEjCAYAAADDry0IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VOX1+PHPyWTfWBIIIeyLCMiOoLiBWxEVXFtxqbZa22+12lZrtbVutVVb17r116p1FxGtRUURlQDusiuy72ENO0nINjm/P547YTJMSAIZZgLn/XrNi5l779w5c0Oek+e5955HVBVjjDFmf+KiHYAxxpjYZ8nCGGNMnSxZGGOMqZMlC2OMMXWyZGGMMaZOliyMMcbUyZKFOSKIyFUi8mm04wAQkbtE5OVox2FMQ1iyMA0mIqtEZI+IFInIdhF5T0TaR/gz80Xkmgjtu5OIqPd9irzvd2skPutQEpHhIlIV9L2KROSdQxyDJcbDhCULc6DOVdV0IBfYBDwe5XgaQ3PvO10E/ElEzoh2QI1gvaqmBz3ObegORCQ+EoGZpsWShTkoqloKTAB6BZaJSDMReVFECkVktYjcLiJx3ro47/VqEdnsbdfMW5csIi+LyFYR2SEi34hIjoj8BTgJeML76/gJb/ujRWSKiGwTkcUi8sOgGLJEZKKI7BKRr4GuDfhOM4EFQP+g/d0qIstFZLeIfC8i5wetu0pEPhWRB72e1koROStofWcRmea9dwqQHfx5IjJaRBZ43zlfRHoGrVslIr8TkfkiUiwiz3rH5H1vfx+JSIv6freg/SaJyKMist57PCoiSd664SJSICK/F5GNwH+85eeIyFwvzs9FpG/Q/n4vIuu8mBaLyGkiMhL4A/Aj7+c2r6FxmhiiqvawR4MewCrgdO95KvAC8GLQ+heB/wEZQCdgCXC1t+6nwDKgC5AOvAW85K37OfCOt08fMAjI9NblA9cEfUYasBb4CRAPDAC2AL289eOA8d52xwDrgE9r+T6dAAXivdfHASXA+UHbXAy0xf2B9SOgGMj11l0FVAA/8+L+P2A9IN76L4CHgSTgZGA38LK37ihvX2cACcAt3vFJDDrWXwI5QB6wGZjtfd9k4BPgzlq+13CgoJZ193j7bQ20Aj4H/hz0vkrgAS/mFO/zNgNDve94pRdbEtDD+1m0DTqeXb3ndwW+qz2a9iPqAdij6T28RqII2OE1kuuBPt46H1AeaLS9ZT8H8r3nHwO/DFrXw9tHPC6RfA70DfOZ+dRMFj8CZoRs8/+AO70YKoCjg9b9lbqTxQ5gj/f8wUBjX8t75gJjvOdXAcuC1qV6+2gDdPAa3rSg9a8GJYs/AeOD1sXhEtvwoGN9WdD6N4Gng17/Cni7lhiHA1Xe9wo8fuitWw6MCtr2B8CqoPeVA8lB65/GSyZByxYDpwDdvERyOpAQso0li8PkYcNQ5kCdp6rNcX/dXg9ME5E2uCGWBGB10LarcX8Vg/vrPHRdPO4v55eAycA4b2jkbyKSUMvndwSGekMiO0RkB3AZroFu5e1zbcjn1CUb19u5CddgVn+2iPw4aAhmB663EjyctDHwRFVLvKfp3vfdrqrFtcRS43ioapUXd17QNpuCnu8J8zp9P99pvao2D3qMD/e53vO2Qa8L1Q0xBnQEbgo53u1xvYllwK9xiWGziIwTkeB9mcOAJQtzUFTVr6pvAX7gRNxQUAWucQnogPtrGVwvJHRdJbBJVStU9W5V7QUMA84Bfhz4qJCPXgtMC2kI01X1/4BCb5/BV2h1aMD3eRgoBX4JICIdgX/jkmKWlyS/A6Qeu9wAtBCRtFpiqXE8RES8uNcRWeF+DuuDXoc73n8JOd6pqvoagKq+qqonevtU3BBWuP2YJsqShTko4owBWgALVdWPO1fwFxHJ8Bra3wKByydfA37jnfRNxw0Pva6qlSIyQkT6iIgP2IVLOlXe+zbhznMEvAscJSJXiEiC9zhWRHp6MbwF3CUiqSLSCzfG3hD3A7eISDLuvIfikhAi8hNcz6JOqroamAncLSKJInIiEHxF0njgbO+EcAKuV1OGG46LpNeA20WklYhkA3ew92cUzr+BX4jIUO9nniYiZ3s/4x4icqp3grwU19sJ/rl1Eu8CB9N02Q/QHKh3RKQI16j/BbhSVRd4636FO2m7AvgUN0b/nLfuOdxw03RgJa5x+ZW3rg3uyqpdwEJgmrctwGPARd7VRv9Q1d3AmcAluL+IN7L3hCy4XkC6t/x5vCt6GuA9YDvwM1X9HngId6J6E9AH+KwB+7oUd2J4G+6cyouBFaq6GLgcd+nxFlwiOVdVyxsYb0Pdi0ti84FvcSfN761tY3VXiP0MeAJ3XJbhztWAO+b34+LfiDtpfpu37g3v360iMrtRv4E5pAJXaxhjjDG1sp6FMcaYOlmyMMYYUydLFsYYY+pkycJEnVfiYnug3ISpm3cPyrXe81Wyt2TK+7K3aGCFiJQHvf5ndKM2TZklCxNVItIJV/dJgdGH+LObcoG8QcBMEWkFVKjqTgBVPcu73yQdeAX4m+4tIviL0J008WNgDiFLFibafoyrUfQ8IfdCiEiKiDwkrujgTnHF+lK8dSd6xex2iMhaEbnKW16jlLmEzGMhrhT5dSKyFFjqLXvM28cuEZklIicFbe8TkT/I3iKCs0SkvYg8KSIPhcQ7UUR+E/oFReRpEXkwZNn/ROS33vN9ivDt74B5N+71xt0YOBiYs7/tQ957utcT+YO4IoH/9paPFpF53vH8VESOCXpPOxH5r7jCkCtF5LqgdceJyGzv2G0Skb/XNxbTxES73og9juwH7nr9X+L+Uq4AcoLWPYmrCZWHq/c0DHdNf0dcMb6xuJIcWUB/7z351KwhdRVBNaFwPZgpQEsgxVt2ubePeNxNcRvx6iIBv8Pdh9ADd8d2P2/bIbj7O+K87bJxxQdzwnzHk3F3QAcuVW+Bu3GtLfspwhdmP91x9Z124e5Q38Hem+B2AFeEbP88cG/IstO99/4VSMQVCTwWd//Isd5x/imudlQi7g/KubjqsYm4OlCrgNO8/X0DjPWeZwBDo/1/yh6ReVjPwkSNdzdzR1whvVm4BupSb10crtG6UVXXqSvD8bmqlnnbfKSqr6krEbJVVec24KPvU9VtqroHQFVf9vZRqaoPsbeSKsA1wO2qulided62XwM7gUAv4BJcscRNoR8GzMAlqUCP5SLgC1VdjyuTkgT0EpEEVV2lqsvDBa2qS9WVGvkHLqm1wFX07aau/MZL4d4XRiVwl6qWe8fgWuApVf3GO86BGyiPBY7HVf79q7f9MuBZ7/uCS/DdRSRLVXer6lf1jME0MZYsTDRdCXyoqlu816+ydygqG1ekMFzD2b6W5fUVXGAQEblZRBZ6Q107gGbsLRK4v896Adcrwfs3bGOtqoormT7WW3Qp7nwC2oAifIFhN9zd0ffgehg9gQUiMmG/37imTVrzDvGOwO+lZpHAXFyPriPQIWTdLbi77cGViO8FLBaRr0VkVAPiME2IndwyUeGde/gh4PPGzsH9hd1cRPrhhn5KcZMWhU6asxY3DBROMa5EeECbMNtUly3wzk/cgushLFDVKhHZzt4igWu9GL4Ls5+Xge+8eHsCb9cSE7haTB+KyP240h/Vkyep6qvAqyKSiSuz/gBwxT5Bqw4TV9k3X1WPFpEbgVaqevt+PjeccEUC71bVB0I39I7PUlXtGbrOi2kxcInXE7wYeFNEWmjNirXmMGA9CxMt5+GGYHrhZqTrj2twZwA/Vleq+zngYRFp651oPt67vPYV4HQR+aGIxIubFS8wq91c4AJxBQS7AVfXEUcGblimEIgXkTuAzKD1zwB/FpHu4vQVkSwAVS3Ajdm/BLwZGNYKR1Xn4GonPQNMVtUdALL/InzhDGLvCe2BuPpOB+vfwHXiCjGKiKSLyLniKuV+AZSLyE3iZjL0iSv2OMiL/woRyfZ+XjtxiWh/8ZsmypKFiZYrgf+o6hpV3Rh44ArVXSbuks6bcT2Mb3BF+B7AnVBeA4zCjdtvwyWIft5+H8FN3LMJN0z0Sh1xTAY+wI39r8Y12MHDVA/jKsN+iBv2eRZ3UjjgBVxhwfqcL3gVd4L51aBl+yvCF84gXNE/cMliVj0+d79U9Uvc7H5P44oELsEbXlPVStyxHoI7sb0F1/sJJNRRwEIR2Y2bMOpHGvkiiCYKrJCgMQdBRE7GDUd1VPtlMocx61kYc4DEzT9xI/CMJQpzuLNkYcwBEJGeuHsbcoFHoxyOMRFnw1DGGGPqZD0LY4wxdTps7rPIzs7WTp06HfD7i4uLSUtLa7yAIiDWY4z1+MBibCwWY+OIhRhnzZq1RVVb1blhtOuNNNZj0KBBejCmTp16UO8/FGI9xliPT9VibCwWY+OIhRiBmWq1oYwxxjQGSxbGGGPqZMnCGGNMnQ6bE9zGGNMYKioqKCgooLQ08rUQmzVrxsKFCyP+OQDJycm0a9eOhISEA3q/JQtjjAlSUFBARkYGnTp1wk1KGDm7d+8mIyMjop8B7kKmrVu3UlBQQOfOnQ9oHzYMZYwxQUpLS8nKyop4ojiURISsrKyD6i1ZsjDGmBCHU6IIONjvZMNQh5k3Zq6l3F9F56w0dpVWsmZbMace3ZpurSPf1TXGHL4sWRxGPl64id9NmL/P8vfmb+Dt6044LP9aMuZw5PP56NOnD6qKz+fjiSeeYNiwYaxatYqePXvSo0cPysvLGTx4MM8++ywJCQnk5+czZsyY6nMS2dnZfPTRR40WkyWLw0RphZ+73llA11ZpPHfVsazdtofMlHjmrNnBnRMX8MmizZzWMyfaYRpj6iElJYW5c+cCMHnyZG677TamTZsGQNeuXZk7dy5+v58zzjiD8ePHc9lllwFw0kkn8e6770YkJksWh4l/TlvO2m17eOWaoXTMSqNjlqs30zM3k+c+W8nDU5Zw6tGtKausYteeCgCapyaSGG+nrYyJZbt27aJFixb7LPf5fAwZMoR169YdkjgsWTRxpRV+npq6jKenLeecvrmc0C27xvoEXxy/OrU7N78xjwuf/pzv1u+ivNJNkZyeFM8pR7Xi3H65nNGrDb44G6YyJtjd7yzg+/W7GnWfvdpmcue5vfe7zZ49e+jfvz+lpaVs2LCBTz75ZJ9tSktL+eqrr3jssceql82YMYP+/d109BdffDF//OMfGy1uSxZNjKoyecFGnpi6jN2llezaU8H2kgrG9G/L3aPD/wc8r39bXvh8FVuLy7l8aEe6tHK9jgXrd/LRws289+0GOmalMrBDC1ZsKaZbq3T+flFf4ix5GBMVwcNQX3zxBT/+8Y/57rvvAFi+fDn9+/dn5cqVnH322fTt27f6fTYMdYSZvWY7N4+fx8hj2nDVsE60zkxmd2kFX2+s5Ln/fMP0JYUclZPOgPbN8cXFceGgPIZ1za51f/G+ON751Ylh191b5ZLPs5+u5MsVW2mdmcybswvo0Sada0/uytaiMgqLyuiRk2EnyM0Rp64ewKFw/PHHs2XLFgoLC4G95yy2bNnCCSecwMSJExk9enTE47BkEUGqSpVS6/DOU/nLeGNmAa///DhaZyQDUFWl3D1xARt3lfL0tOU8lb+8xntapu3kT+f04srjOxLvO/jzDb44YVSfXEb1ya2O+ZevzOZvHyxme0kFL3+xmt1llQzo0JxLh3SgZ24mO0oq+NeMFcwv2MGwrlmMPCaXs733z127gzdmruW3ZxxFVnrSQcdnzJFu0aJF+P1+srKyKCkpqV6enZ3N/fffz3333WfJoilTVX7z+lwWbdzN+zeehIjw2bItjJ+5lh8d255FG3bztw8WA3D/pEU8/CM3zvjutxuYV7CThy7ux6COLXjv2w2UV1aR4BMSd67h6jGnRvTcgohw/wV9mV8wg6fzl3PyUa04uXs2L36xusZludnpSZxyVCu+XLGVSd9u5JkZK+icVM6kKZ9T4Vfmrt3Bqz87jmYpB1aHxpgjWeCcBbi25IUXXsDn8+2z3Xnnncddd93FjBkzIh6TJYsIGffNWt6eux6A79btok+7Zvzj46V8tXIb//OWj+zdhk7Zafxz2nIuGdKBPnnN+PvkRfTMzeS8AXn44oTrRnSr3md+/rpDchK6WWoCr1wzlLXbSzixWzYiwk9O6MyyzUWsKCyisko5o1cOyQk+qqqUd+av5973FjJ/dwWn92zN6P553DR+Llc//w3PXnWsJQxjGsjv94dd3qlTp+pzF+D+uJs3b1716+HDh0csJksWEbBk027umriAIZ1aMnvNdiZ9t4FWGUl8vWobvxzelfYtU1m5pZibzjwKf5Uyce46fvnKLPaU+yku9/PS1X2ifmVSp+w0OmXvne7RFyf0aJNBjzY17wSPixPG9M/j1KNb88K707nuwsGICPFxwg2vzeGcx2fw+NiBtEhNYEtROf3bN4/6dzPGNJwliwh44P1FpCXF88RlA7hp/Dze/3YDWWmJqMLFg9vTObvmnLt/uaAP9777PWf2bsM5fXIZ1q32k9WxKiM5gWOyfdUnwUf1ySUnM5nrX53NeU9+Vr3d8V2yeGxs/+pzNMaYpsGSRSMrKqtkxtItXHF8R1pnJDOqTy63vfUt/5y2gj55zfZJFAAjerRmRI/WUYg2sgZ1bMF7N5zE+JlraZGaQEm5nwc+WMSoxz7l9J6t6dY6nTH982iVYSfCTWxR1cPu6j833faBs2TRyKYtLqTcX8WZvVxpjR/0bsPtb3/HlqIyrj35wOrIN2Ut0xL5xSldq18P65rNn9/9ninfb2LcN2t57OOl3Hhad8oqq5i+pJCiskpE3HZXDetE2+YpNfa3aVcp0xYX0jM3kz7tmh3qr2OOAMnJyWzduvWwKlMemM8iOfnAe/SWLBrZ5AUbaZmWyOBOLQHXWB7XpSWfLdvKOX3bRjm66OvRJoOXrxkKwLLNRdw1cQH3vudmCjsmL5PcZsnsqfDz7Kcree7TlRyT14wu2WmUlPtZXljE0s1FAHRomconN53SKJcPGxOsXbt2FBQUVN/XEEmlpaUH1YA3RGCmvANlyaIRlVdWMXXRZs7qU7N0xu9+cDQje+/Y56/kI1231um8dPUQ5q7dQW6zFNo02/tLs3ZbCS9/tZpvC3byxYqtpCT66JKdznkD8khO8PHnd79n0ncbGd3PErBpXAkJCQc8m1xD5efnM2DAgEPyWQfLkkUj+nLFVnaXVXJmrzY1lvdv35z+7ZtHKarYJiIM6LBvkbT2LVO57ayeYd9TVaW89vUans5fzrl9c6uHCsoq/cSJkLCf3oaq8rcPFvHq12v40bHtGXtsB9KT40lLjCclcd/r2I0xjiWLRvT23HWkJPg4sXvTu5qpKYmLE35xSldufmMeD364mDXb9jC/YAdrt5XQo00mb183jKT48A3/OysqeGvpcnrmZvLv6Sv4f9NWAJCW6OPFq4cwqGPLQ/lVjGkyLFk0krdmF/DW7HX85IROJCfYX6iRNqZ/Wx6ZsoQnpy4nOz2RoZ2zOKl7Ni9/uYYnPlnGTWf2AFxP4p/TVvDGrLVUVSmrtlZwwYA8Hry4HwXb9zB9aSGqyjOfruTnL81m4vUn1Gu40F/l7lL/csVWurZK56Tu2aQluV+n4rJKVm0tprTCDwi9cjOt12KaPEsWjWDOmu3c+ta3DO3ckj+MCj90YhpXgi+OV382lG3F5fRr17y6Qm5JuZ+n8pfzg95t6Jmbyd3vLODFL1YzpHNLcjKT6dO8gr95FXU7ZKVyeVZHAI7rksX5T33OFc9+xbGdWuKLEzq0TKVjVioJvjiKyiqZtqSQL5ZvpayyirIKdwPl3niEjOQE/FXKTm++kIBOWak8edlAere1q7dM02XJ4iCVVvi5cdxcWmck8fTlg/Y7Xm4aV/AkTwF3nNOLGUu3cM7jnyICqvDzk7tw61lHIyLk5+eHvYKqe04GT142kDv+9x1TF2+mrLKKHSU1G/1mKQmc1D2bFqmJ+OKEgR1bcGK3bBZv3M20JYUUl1UC0KZZMp2z00hPimfHngr++t5Czn/qc+6/oA8XDDzwq1GMiSZLFgfpn9OWs2ZbCa9eM5SWaYnRDueI1zw1kRd/OoRJ324A4KicDM6t5xVTpxzVimm/G1H9emdJBWu3l1ClSnxcHEflpIdNNMd3zeL4rlm17veErln86rU5/Hb8PAp3l/HzoPtOjGkqIposRGQk8BjgA55R1ftD1ncEngNaAduAy1W1wFvXAXgGaA8oMEpVV0Uy3oZau62Ep/PdDHVNsUTH4apnbiY9czMPej/NUhNolnrwQ0dZ6Uk8/5Mh/Hb8XO57fxGVVVqjQKQxTUHExkxExAc8CZwF9ALGikivkM0eBF5U1b7APcB9QeteBP6uqj2BIcDmSMV6oO5/fxG+OOH2s0O/ljE1JcbH8Y9LBnB231we/WgJywuLoh2SMQ0SyQH2IcAyVV2hquXAOGBMyDa9gMDkslMD672kEq+qUwBUtUhVS4ghO/dU8OH3G7l0SIcaN5MZU5u4OOGuc3uTnODjrokLDrpWjzGHkkTqP6yIXASMVNVrvNdXAENV9fqgbV4FvlLVx0TkAuBNIBs4CbgGKAc6Ax8Bt6qqP+QzrgWuBcjJyRk0bty4A463qKiI9PT0em//2boK/v1tOX86LpmuzQ/NZZENjfFQi/X4IDZinLK6glcWlnP1MYmcmBe/T/2hWIixLhZj44iFGEeMGDFLVQfXuaGqRuQBXIQ7TxF4fQXwRMg2bYG3gDm4cxsFQHPvvTuBLrjzKm8CV+/v8wYNGqQHY+rUqQ3a/urnv9bj//qRVlVVHdTnNkRDYzzUYj0+1diIsaLSr+c+PkM7/v5dHf3Ep/r8Zyt1xpJCXbe9RKuqqmIixrpYjI0jFmIEZmo92vRInuBehzs5HdDOW1ZNVdcDFwCISDpwoaruEJECYK6qrvDWvQ0cBzwbwXjrbVdpBdOXuDLkh0tVSnPoxPvieP3a45kwu4BnZ6zgzokLqtelJPjonKlUtdnE8KNaV98/Yky0RTJZfAN0F5HOuCRxCXBp8AYikg1sU9Uq4DbclVGB9zYXkVaqWgicCsyMYKwN8snCzZT7qxjVp03dGxsTRkqijyuO68jlQzuwaVcZK7YUsaKwmOWFRfxv1mp++vxMOrRM5cxeOXTPSWfV1hIq/VWcclRrhnRuSWJ8+NONm3eV8uTUZRSX+0lOiOO8/nnVFZCNORgRSxaqWiki1wOTcZfOPqeqC0TkHly3ZyIwHLhPRBSYDlznvdcvIjcDH4v7030W8O9IxdpQk77dQJvMZAa037cAnjENISK0aZZMm2bJDOvqLr8+IW0zRS2O4r9z1vHiF6sp91cRHyfExQn/nrGSvOYpTLz+BLLSa04aparcPGE+XyzfQuuMZHaUlPPyl2sY1LEFT1460C7EMAclovdZqOokYFLIsjuCnk8AJtTy3ilA30jGdyCqqpQvlm/lnH5tbYjARER8nHDegDzOG5BHUVklW4vKyGueQoVf+WTRZn79+hzufud7/jG2ZmnrN2YVMH1JIXeP7s2VwzpRUl7JGzMLeOCDRdwwbg6v/ey4sPOf7y6tIDUxHl+csLywiEemLGHTrlIE4eLB7bh4cPt93mOOPHYHdwMt3VzE7rJKBnW0XoWJvPSkeNK9AoXxPji7by7LNhfxyEdLOLtvLgPaN2d5YTErthRx//uLGNKpJVcc5+pdpSbGc+WwTmQkx/Pb8fN4/JOl/Pr0o6r3vbu0gkc/Wsrzn6+iWUoCAzs0Z9qSQpITfPTJa8aWojJ+N2E+63eUcsNp3ez83BHOkkUDzV6zHYCBHWx+ChMdvxzRlQ8WbOTnL82qsbxVRhIPeEUSg10wsB2fLt3CYx8v5ZWv1lQvLy6rZE+Fn4sGtqPS6zGf268tt53Vk1YZSVT4q/j9m/N55KMlbC0u485ze4ftmZgjgyWLBpq9ejstUhPonJ1W98bGRECCL47Hxw7gta/X0L5FCl1apdOlVRptm6XUOjT65/OOIbd5MtuK9xZHjI8TLhzUrtaJuRJ8cTx4UT+y05P41/QVbC0q5+Ef9at1rhBzeLNk0UCz12xnYIcW1iU3UdWtdTp/Oqf+ZWbSkuL53Q+ObvDnxMUJfxjVk+z0RP46aRGrtxVz73l9Grwf0/RZPe0G2FFSzvLCYgba+QpzhLn25K788/KBbN5VxvlPfcbjc0p5c1bBPnN3gCvbv624PApRmkiynkUDzFmzA4ABdr7CHIFGHpPLCd2yeWLqMl7/ciU3vTGP1hlJPD52AEO7uBLtG3bu4crnvmZFYTGj+7dl7JAOHJWTQbOUhBr7qvRXUbB9D5VVSrfWe8tdFJdVMmPpFpZu2s3PTu5is07GEEsWDTB7zXbiBPq1s2RhjkwZyQncdlZPhiZvJLNzP26ZMJ9Ln/mKiwe1o3tOBs/OWMGu0kouHNiOd+av563ZrmhDdnoiXbLTyUyJZ+WWYtZsK6HC7+rS/fSEztx4enf+37TlPPvpSsoqqwAQgetP7R6172pqsmTRALPXbKdnbmb1XMvGHKniRBjcqSX/u/4E7py4gHfnb6CobC3Z6UmMu/Y4jslrxh9G9eTrVdtYUejuTl+xpYi12/bQrXU6Z/ZuQ5fsNL5dt5PnPlvJy1+6mw/PH5DHxYPb8dynq/jntBVcMqQD2SE3H5rosFavnvxVytw1O2xaTGOCZCQn8PAP+6OqFO4uIyM5gZREN3TULDWBM3rlADm1vv/iwe05rksWb8xcyy9HdONYrzRJTmYyZz4ynX98vJR7xhxzKL6KqYMli3pavHE3xeV+Bna0IShjQokIrTMPrJzIqD65jOqTW2NZ11bpjB3Snle+WsPabSX0zM3kuhHdrFcfRXbk62nvzXh2JZQxh8JNZ/SgtKKKBet3MXXxcgBuGdnwy39N47BLZ+tp9prtZKUl0qFlarRDMeaI0CItkQcv7sf7N57Euf3a8vznq9haVBbtsI5Ylizqac6aHQzsaDfjGRMNN57WndIKP/+aviLaoRyxbBiqHrYWlbFySzE/tOqbxkRFt9bpjO7Xlhe/WE2CL46URB9jh3SgZVpitENMrSw/AAAcqElEQVQ7YljPoh4CN+NZ8UBjoufG048iMT6Op/KX8ffJi/nVa7OpqtJoh3XEsGRRD7PXbCc+TuhrN+MZEzWds9OYd+eZrLjvbB64sA+fLdvK09OWRzusI4YNQ9VD4Ga8wPXjxpjo+uHg9sxYuoWHpywhNdHHj45tz55yP58s2szHCzczfWkhJ7eNY/jwaEd6+LBkUQ8LN+zm7L65dW9ojDkkRIS/XtCHzbvKuPud73nowyUUl1eiCjmZSRzdJoMPVu3go+83cXqvmjcFVvir+HjhZjJT4qunsjV1s2RRh7JKPzv3VJB7gDccGWMiIzM5gdd/fhyzVm/n9W/W0rZ5Cmf0yqF320zK/VWc+bcP+d2Eebx09VB6tMlg9dYSJi/YyKtfrWHdjj0A/KB3Dr86tTvdWqdb0cI6WLKow5YiV2q5VYbVpzEm1ohXo2qwVyYkICnex//1S+Ker8o55/FPiRMInAsf0rkld57bi2WFRfzj46VMXrAJgL7tmnH1iZ0Z1SeXBJ+dzg1lyaIOhbvdTUCWLIxpWtqkxTH51yfz9cptrNpaTOuMJE7rmUPb5ikAnAlcOLAdX6/cxvLCIibOW8+N4+by+CfLePLSgfRokxHdLxBjLFnUwZKFMU1X+5aptN9P1YWczGTO7dcWgBtO7c6H32/i9re/Y8yTn3LveX24aJArHOqvUtZuK0Fxc26s2FJMpb+Kc/q2JTH+yOiFWLKogyULY44McXHCyGPaMLBjc254bQ43vzGPr1duZXS/PO5973sWbdy9z3uenLqMG08/it2lFewoqeC4Li3p374FvlrmQm/KLFnUIZAsstIsWRhzJGidkczLVw/lsY+X8vgnyxg/s4C85in8eUxvMpITSE6Io1N2GgXb9nD3uwu44bU5Nd6fnhRPcoKPFqkJPH35QLq1PjyGsyxZ1KGwqJQWqQlHTFfTGAPxvjhuOrMHQztn8e26nVw5rCOpiTWby6PbZHJi92y+XbeTts1TSEv0MX3pFmau2kZllfLuvPXc/vZ3vPaz4w6LmnKWLOpQuLvMhqCMOUKd2D2bE7vXfi9GcoKvesImgNH92jLaOwfSMzeTP739He/M31C9rCmzP5frYMnCGHMgLh3SgWPyMrn33e/ZXlwe7XAOmiWLOhQWldHK5gA2xjSQL06497w+bC9x93rMXbsj2iEdFEsW+xGYV9h6FsaYA9G/fXMm/GIYABc+/TkjH53OjePmVN9B3pRYstiP4nI/pRVVliyMMQesX/vmTLrhJH5+chfaNk/hwwWb+M3rc5tceXU7wb0fdo+FMaYxNEtNqJ4/fPw3a7nlzfm8/NVqOkQ5roawnsV+VCeLdCsiaIxpHBcPbsfJR7Xi/vcXUVhSFe1w6s2SxX5Yz8IY09hEhPsu6IO/SnlvZUW0w6m3iCYLERkpIotFZJmI3BpmfUcR+VhE5otIvoi0C1mfKSIFIvJEJOOsTeHuUgCy022eX2NM48lrnsL5A/L4fF1lk7msNmLJQkR8wJPAWUAvYKyI9ArZ7EHgRVXtC9wD3Bey/s/A9EjFWJfCojJ8cUKLVEsWxpjG9ZMTOlNeBa9+vSbaodRLJHsWQ4BlqrpCVcuBccCYkG16AZ94z6cGrxeRQUAO8GEEY9yvwt1lZKcnEncYFgUzxkRXjzYZ9M6K48UvVlHhj/1zF5FMFnnA2qDXBd6yYPOAC7zn5wMZIpIlInHAQ8DNEYyvTnaPhTEmks7slMCmXWWc9+Rn3PzGPJYXFkU7pFqJamSu9RWRi4CRqnqN9/oKYKiqXh+0TVvgCaAzbrjpQuAY4HIgVVX/JiJXAYOD3xf0/muBawFycnIGjRs37oDjLSoqIj09vcayOz/fQ7Mk4beDYuNqqHAxxpJYjw8sxsZiMTaOXbuLyN+cyMKtflburCIrRbhrWAoJQaMZK3f6yUwUslIi87f9iBEjZqnq4Do3VNWIPIDjgclBr28DbtvP9ulAgff8FWANsArYAuwC7t/f5w0aNEgPxtSpU/dZNuQvU/R3b8w9qP02pnAxxpJYj0/VYmwsFmPjCI7x44UbtePv39WHJi9SVdVNO/foDa/N1o6/f1d73D5JH/94iZZWVO6zj9KKSl2ztfiAYwBmaj3a9EjelPcN0F1EOgPrgEuAS4M3EJFsYJuqVnnJ5DkvgV0WtM1VuJ7FPldTRVJVlbKlqNyGoYwxh8SpR+dwwYA8nspfztertjFz1Xbi4oTrR3RjeWERD364hLdmr+OeMcfQMzeDqYsL+ej7TUxfWkjP3Eze/L9hEY0vYslCVStF5HpgMuADnlPVBSJyDy6TTQSGA/eJiOKGoa6LVDwNtb2kHH+VWhFBY8whc8e5vZi5ejvbisu59uQu/HBwezplpwGQv3gzd05cwOXPfoUIqEKbzGTOH5DHGb1yIh5bRMt9qOokYFLIsjuCnk8AJtSxj+eB5yMQ3n4VFgVuyIuN8xXGmMNf89REpt8yIuy64T1aM/nXWbz85WqKy/yc1rM1vdtmHrKJlaw2VC3s7m1jTKxJTvBxzUldovLZVu6jFpYsjDFmL0sWtbBkYYwxe1myqEXh7jJSEnykJfqiHYoxxkSdJYtaFBa5u7cP1ckjY4yJZZYsamGlPowxZi9LFrUo3F1m91gYY4zHkkUtAsNQxhhjLFmEVV5ZxY6SCrKtZ2GMMYAli7C2Fttls8YYE8ySRRh2j4UxxtRkySIMSxbGGFOTJYswLFkYY0xNlizCCCSL7PTEKEdijDGxwZJFGIVFZTRLSSAp3kp9GGMMWLIIy+7eNsaYmixZhGF3bxtjTE2WLMKwu7eNMaYmSxZh2DCUMcbUZMkiRKW/ipJyP5nJCdEOxRhjYoYlixDl/ioAkhLs0BhjTECdLaKIdBaR5KDXKSLSKZJBRVN5pUsWiT5LFsYYE1CfFvENoCrotd9bdlgqCySLeEsWxhgTUJ8WMV5VywMvvOeH7a3NgZ5FkiULY4ypVp8WsVBERgdeiMgYYEvkQoou61kYY8y+4uuxzS+AV0TkCe91AfDjyIUUXWWVfsB6FsYYE6zOZKGqy4HjRCTde10U8aiiaO8wlNWFMsaYgPpcDfVXEWmuqkWqWiQiLUTk3kMRXDSU2zCUMcbsoz4t4lmquiPwQlW3A6MiF1J02TkLY4zZV31aRJ+IVNe+EJEU4LCthWFXQxljzL7qc4L7FeBjEfkPIMBVwAuRDCqaAndwW8/CGGP2qs8J7gdEZB5wOqDAZKBjpAOLlsDVUHYHtzHG7FXfFnETLlFcDJwKLIxYRFFWPQyVYFdDGWNMQK09CxE5ChjrPbYArwOiqiMOUWxRYbWhjDFmX/trERfhehHnqOqJqvo4ri5UvYnISBFZLCLLROTWMOs7isjHIjJfRPJFpJ23vL+IfCEiC7x1P2rI5x4MuxrKGGP2tb8W8QJgAzBVRP4tIqfhTnDXi4j4gCeBs4BewFgR6RWy2YPAi6raF7gHuM9bXgL8WFV7AyOBR0WkeX0/+2CU2dVQxhizj1pbRFV9W1UvAY4GpgK/BlqLyNMicmY99j0EWKaqK7zig+OAMSHb9AI+8Z5PDaxX1SWqutR7vh7YDLSq/9c6cDYMZYwx+6qzRVTVYlV9VVXPBdoBc4Df12PfecDaoNcF3rJg83A9GIDzgQwRyQreQESG4KrcLq/HZx60ssoqEnxCXFy9O1HGGHPYE1WNzI5FLgJGquo13usrgKGqen3QNm2BJ4DOwHTgQuCYwB3jIpIL5ANXquqXYT7jWuBagJycnEHjxo074HiLiopIT0/n1YVlTC+o5J9npB3wviIlEGOsivX4wGJsLBZj44iFGEeMGDFLVQfXuaGqRuQBHA9MDnp9G3DbfrZPBwqCXmcCs4GL6vN5gwYN0oMxdepUVVX943/n64B7PjyofUVKIMZYFevxqVqMjcVibByxECMwU+vRxkZyYP4boLs3LWsicAkwMXgDEckWkUAMtwHPecsTgf/iTn5PiGCM+yirqLLzFcYYEyJiraKqVgLX4+74XgiMV9UFInJP0GRKw4HFIrIEyAH+4i3/IXAycJWIzPUe/SMVa7ByfxVJCZYsjDEmWH1qQx0wVZ0ETApZdkfQ8wnAPj0HVX0ZeDmSsdWmvNJ6FsYYE8paxRBlldazMMaYUNYqhrCehTHG7MtaxRDllVVW6sMYY0JYqxiirNJv828bY0wISxYhyqxnYYwx+7BWMUS535KFMcaEslYxRFlFlVWcNcaYENYqhij3W7IwxphQ1iqGsEtnjTFmX9Yqhiir9Nv828YYE8KSRQjrWRhjzL6sVQxS6a+iSm3+bWOMCWWtYhCbf9sYY8KzVjFI9fzbliyMMaYGaxWDlPstWRhjTDjWKgYpqwgMQ9nVUMYYE8ySRZByvx+wnoUxxoSyVjFI4AS3XTprjDE1WasYpPpqKJspzxhjarBWMUjgaqgk61kYY0wN1ioGKbNLZ40xJixrFYNU9yzsaihjjKnBkkUQuynPGGPCs1YxSFmlXTprjDHhWKsYpNxqQxljTFjWKgaxch/GGBOetYpBAuU+LFkYY0xN1ioGCfQsbBjKGGNqslYxiJX7MMaY8KxVDFJW6SfRF4eIRDsUY4yJKZYsgpRXVtkQlDHGhGEtY5Dyyio7uW2MMWFYyxikzJKFMcaEZS1jEBuGMsaY8CLaMorISBFZLCLLROTWMOs7isjHIjJfRPJFpF3QuitFZKn3uDKScQbYMJQxxoQXsZZRRHzAk8BZQC9grIj0CtnsQeBFVe0L3APc5723JXAnMBQYAtwpIi0iFWtAWaXfkoUxxoQRyZZxCLBMVVeoajkwDhgTsk0v4BPv+dSg9T8ApqjqNlXdDkwBRkYwVsDdlGflyY0xZl/xEdx3HrA26HUBrqcQbB5wAfAYcD6QISJZtbw3L/QDRORa4FqAnJwc8vPzDzjYoqIiCrfuwScc1H4iqaioKGZjg9iPDyzGxmIxNo6mEGNAJJNFfdwMPCEiVwHTgXWAv75vVtV/Af8CGDx4sA4fPvyAA8nPzyc5LZ4WqYkMHz7kgPcTSfn5+RzMd4y0WI8PLMbGYjE2jqYQY0Akk8U6oH3Q63besmqquh7Xs0BE0oELVXWHiKwDhoe8Nz+CsQJ2NZQxxtQmki3jN0B3EeksIonAJcDE4A1EJFtEAjHcBjznPZ8MnCkiLbwT22d6yyLKroYyxpjwItYyqmolcD2ukV8IjFfVBSJyj4iM9jYbDiwWkSVADvAX773bgD/jEs43wD3esoiym/KMMSa8iJ6zUNVJwKSQZXcEPZ8ATKjlvc+xt6dxSJRV2tVQxhgTjv0ZHaS80m/nLIwxJgxrGYPYMJQxxoRnLaNHVb2b8uyQGGNMKGsZPX4FVZslzxhjwrGW0VPhZlQlOcFOcBtjTChLFp6SCgUgIznaN7UbY0zssWTh2VPp/k23ZGGMMfuwZOHZUxnoWSREORJjjIk9liw8e5OF9SyMMSaUJQtPiTcMlZFkycIYY0JZsvDsqbBhKGOMqY0lC88evw1DGWNMbSxZePZUQJxAaqLdZ2GMMaEsWXhKKpX0pHhEJNqhGGNMzLFk4SmttPMVxhhTG0sWnpJKtfMVxhhTC0sWnj2WLIwxplaWLDx7bBjKGGNqZcnCs8c7wW2MMWZfliw8eypsGMoYY2pjycJjw1DGGFM7SxZAaYWfSrW7t40xpjaWLIDdpa6KoCULY4wJz5IFUFRmycIYY/bHkgWwu7QCgPQkO2dhjDHhWLLAhqGMMaYuliywZGGMMXWxZMHeYagMG4YyxpiwLFlgPQtjjKmLJQv2Jot0SxbGGBOWJQugqKyCxDhI8NnhMMaYcKx1xPUsUhJshjxjjKmNJQu8ZGEjUMYYUytLFsDuskpS4q1nYYwxtYloshCRkSKyWESWicitYdZ3EJGpIjJHROaLyChveYKIvCAi34rIQhG5LZJx7i6tINV6FsYYU6uIJQsR8QFPAmcBvYCxItIrZLPbgfGqOgC4BHjKW34xkKSqfYBBwM9FpFOkYnXDUNazMMaY2kSyZzEEWKaqK1S1HBgHjAnZRoFM73kzYH3Q8jQRiQdSgHJgV6QCLbJkYYwx+yWqGpkdi1wEjFTVa7zXVwBDVfX6oG1ygQ+BFkAacLqqzhKRBOAl4DQgFfiNqv4rzGdcC1wLkJOTM2jcuHEHFOsvphRzfI5yZd/0A3r/oVJUVER6euzGGOvxgcXYWCzGxhELMY4YMWKWqg6ua7toj9SPBZ5X1YdE5HjgJRE5Btcr8QNtcYlkhoh8pKorgt/sJZB/AQwePFiHDx/e4AD8VUrpB5PITEnkQN5/KOXn58d0jLEeH1iMjcVibBxNIcaASA5DrQPaB71u5y0LdjUwHkBVvwCSgWzgUuADVa1Q1c3AZ0Cdme9ABOaysGEoY4ypXSSTxTdAdxHpLCKJuBPYE0O2WYMbakJEeuKSRaG3/FRveRpwHLAoEkGqKuf0zSUv3ZKFMcbUJmLJQlUrgeuBycBC3FVPC0TkHhEZ7W12E/AzEZkHvAZcpe4kypNAuogswCWd/6jq/EjE2Tw1kScuHUifVtEekTPGmNgV0RZSVScBk0KW3RH0/HvghDDvK8JdPmuMMSYG2B3cxhhj6mTJwhhjTJ0sWRhjjKmTJQtjjDF1smRhjDGmTpYsjDHG1MmShTHGmDpFrJDgoSYihcDqg9hFNrClkcKJlFiPMdbjA4uxsViMjSMWYuyoqq3q2uiwSRYHS0Rm1qfyYjTFeoyxHh9YjI3FYmwcTSHGABuGMsYYUydLFsYYY+pkyWKvfSZXikGxHmOsxwcWY2OxGBtHU4gRsHMWxhhj6sF6FsYYY+p0xCcLERkpIotFZJmI3BrteABEpL2ITBWR70VkgYjc6C1vKSJTRGSp92+LGIjVJyJzRORd73VnEfnKO56vexNfRTO+5iIyQUQWichCETk+lo6jiPzG+xl/JyKviUhyLBxDEXlORDaLyHdBy8IeN3H+4cU7X0QGRim+v3s/5/ki8l8RaR607jYvvsUi8oNIx1dbjEHrbhIRFZFs7/UhP4YNdUQnCxHx4SZaOgvoBYwVkV7RjQqASuAmVe2FmyXwOi+uW4GPVbU78LH3OtpuxE1uFfAA8IiqdgO246bOjabHcFP0Hg30w8UaE8dRRPKAG4DBqnoM4MPNKBkLx/B5YGTIstqO21lAd+9xLfB0lOKbAhyjqn2BJcBtAN7vziVAb+89T3m/+9GIERFpD5yJmxE0IBrHsEGO6GQBDAGWqeoKVS0HxgFjohwTqrpBVWd7z3fjGrg8XGwveJu9AJwXnQgdEWkHnA08470W3HS4E7xNohqjiDQDTgaeBVDVclXdQWwdx3ggRUTigVRgAzFwDFV1OrAtZHFtx20M8KI6XwLNRST3UMenqh96M3QCfAm0C4pvnKqWqepKYBnudz+iajmGAI8AtwDBJ4wP+TFsqCM9WeQBa4NeF3jLYoaIdAIGAF8BOaq6wVu1EciJUlgBj+L+01d5r7OAHUG/sNE+np1xc7r/xxsqe8ab0z0mjqOqrgMexP2FuQHYCcwito5hsNqOWyz+Hv0UeN97HjPxicgYYJ2qzgtZFTMx1uZITxYxTUTSgTeBX6vqruB13lzlUbuUTUTOATar6qxoxVAP8cBA4GlVHQAUEzLkFM3j6I35j8EltbZAGmGGLWJRtP//7Y+I/BE3lPtKtGMJJiKpwB+AO+raNhYd6cliHdA+6HU7b1nUiUgCLlG8oqpveYs3Bbqm3r+boxUfbu700SKyCjd8dyru/EBzb0gFon88C4ACVf3Kez0Blzxi5TieDqxU1UJVrQDewh3XWDqGwWo7bjHzeyQiVwHnAJfp3vsCYiW+rrg/DOZ5vzftgNki0obYibFWR3qy+Abo7l19kog7CTYxyjEFxv6fBRaq6sNBqyYCV3rPrwT+d6hjC1DV21S1nap2wh23T1T1MmAqcJG3WbRj3AisFZEe3qLTgO+JneO4BjhORFK9n3kgvpg5hiFqO24TgR97V/QcB+wMGq46ZERkJG5YdLSqlgStmghcIiJJItIZdxL560Mdn6p+q6qtVbWT93tTAAz0/p/GxDHcL1U9oh/AKNyVE8uBP0Y7Hi+mE3Fd/PnAXO8xCndO4GNgKfAR0DLasXrxDgfe9Z53wf0iLgPeAJKiHFt/YKZ3LN8GWsTScQTuBhYB3wEvAUmxcAyB13DnUSpwjdrVtR03QHBXFS4HvsVd3RWN+Jbhxv0DvzP/DNr+j158i4GzonUMQ9avArKjdQwb+rA7uI0xxtTpSB+GMsYYUw+WLIwxxtTJkoUxxpg6WbIwxhhTJ0sWxhhj6hRf9ybGHDoiErg8E6AN4MeV7AAYoq6G12FPRGbifj9bAinsvUHrXFVdW+sba+7jfuA9VZ2xn20uAtqr6iMHGbI5zNmlsyZmichdQJGqPhiyXHD/d6vCvjEKRCRe99Zzasz9XoOrpPrrWtb7VNXf2J9rTCgbhjJNgoh0Eze/xyvAAiBXRM4SkS9EZLY370Oat+2xIjJNRGaJyPsikuMt/423j/ki8nKYz+gqIjO8ooOzRGRo0Lo/iMi3IjJPRP7iLftURB7xegHXe5UApnr7n+JV5UVELhE3X8U8EZnqLesjIt+IyFxv+y71PA7JIrJVRB4XkfnAQBG519vXdyLyZNC247waXojIRhG50/tu80Skm7f8FyLyYND2j3rHdIWIjPaWx4vIv8XNFfGBiHwY2K85cliyME3J0bh5Hnrh7oq9FThNVQfi7tC+UUSScDWqLlTVQcDLwJ+9998C9Fc338H1Yfa/AThDXdHBy4B/AIjIubj5Boaoaj/goaD3+FR1sKo+CjwFPOPt/w1cVV6AO704+wHne8t+CTyoqv2BY4H1DTgOLYEpqtpXVb8BHlbVY4E+QGsROaOW9633vtsLQNieCpANDMOVG/mrt+wSb3lP4Brg+AbEag4Tds7CNCXLVXWm93wYbsKqz92oFInAp7gGrTfwkbfchyu1AK5H8rKI/A9X+iNUEvCEiPTDVS3t6i0/HXhOVfcAqGrwHAWvBz0fiitiB/Aie5PUZ8CLIvIGrlggwOfA7SLSEXhLVZfV9yAAe1Q1uIbZmSJyE5CMa9Q/w00EFCrw2bNwhR/D+a+qqojMATp4y04Exqsbsy4QkVrPgZjDlyUL05QUBz0X3Ax4VwRvICIDgPmqelKY9/8AOAUYDfxBRPqGjPffhKstdDmQABQ1MKba/Iy9iWS2iAxQ1ZdE5Avc5FEfiMhP1U2WUx/VRfJEJAM3mc5AVd3gndROruV9Zd6/fmr/3S8DV4JcRGzkwVSz/wymqfocOCUw1i8iaSLSHVe1NU9EhnjLE0Wkt7hpNNup6ie44ahs3Mx0wZoBG7y/oK/EJSRwf6X/VERSvH22rCWmL4Efes8vBwKNfxd1s5/9CTdNap6IdFHVZar6GPAu0PcAj0MqrvHfKiKZ7B3makyfARd5FVHzcD0Nc4SxnoVpklR1k4hcDbwurrw8wB9Udal3Oeg/vMbThzvHsAx41ftLPA53vmB3yG6fACaIyE+B99j7V/a73tDUTBGpAN7BNfyhrgOeE5HbgE3AT7zlj4grjS3Ah6r6nYjcLiJjcede1gN3HcRxeA2XJDfgZlRsbK8BI3DT+64C5uBm9TNHELt01hhTJxFJV9Ui78qyL3EltLdGOy5z6FjPwhhTH1O8YbgE3LwvliiOMNazMMYYUyc7wW2MMaZOliyMMcbUyZKFMcaYOlmyMMYYUydLFsYYY+pkycIYY0yd/j++itFeF8nRTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate mean for the first 150 trees from all BRF classifiers\n",
    "test_brf_mean_accs = np.mean([ accs[:150] for accs in test_brf_accs ], axis=0)\n",
    "\n",
    "# Print out results\n",
    "print(\"Max acc:, {}, Mean acc: {}\".format(max(test_brf_mean_accs), np.mean(test_brf_mean_accs)))\n",
    "\n",
    "# Plot results\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.suptitle(\"Boosted Random Forest\")\n",
    "plt.title(\"Accuracy vs #Trees\")\n",
    "ax1.set_xlabel('Trees across Training')\n",
    "\n",
    "# Accuracy\n",
    "color = 'tab:blue'\n",
    "ax1.set_ylabel('Acc')  \n",
    "ax1.plot(test_brf_mean_accs, color=color, label=\"BRF\")\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Random Forest\n",
    "The following figure shows performances of 3 models, with the first two proposed in the referenced paper[1], while the last as a baseline:\n",
    "- Blue: BRF\n",
    "- Orange: BRF w/out weight update\n",
    "- Green: RF\n",
    "\n",
    "<img src=\"Plots/pred_acc.png\" width=\"500\">\n",
    "\n",
    "### Observations:\n",
    "+ BRF obtains the <b><u>highest optimal accuarcy (>97%)</u></b> with <b><u>just 20~30 trees</u></b>!\n",
    "    - Boosting!\n",
    "+ BRF has better performance with <b>updated weights</b>\n",
    "    - Classification power is not the same for all trees\n",
    "+ RF and BRF w/out weight updates slowly achieve optimal accuracy with more trees \n",
    "    - Yet, lower than BRF's best\n",
    "\n",
    "### Question:\n",
    "+ <u>Why performance of BRF drops significantly when #trees grows?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study on Weighted Errors and Tree Weights\n",
    "Recall that the weighted errors of trees $\\epsilon$ and tree weights $\\alpha$ in BRF are given as\n",
    "- Weighted error (epsilon)\n",
    "<img src=\"Pics/epsilon.png\" width=\"200\">\n",
    "- Tree weight (alpha)\n",
    "<img src=\"Pics/alpha.png\" width=\"200\">\n",
    "\n",
    "From these two definition, we know:\n",
    "- Good prediction --> Low error $\\epsilon$ --> High weights $\\alpha$\n",
    "- Bad prediction --> High error $\\epsilon$ --> Low weights $\\alpha$\n",
    "\n",
    "<br>Even though the under-performing trees are given less significance, <b><u>many small erroneous trees add up to give great influence</u></b> to the final prediction.\n",
    "<img src=\"Plots/eps_vs_alpha.png\" width=\"500\">\n",
    "\n",
    "### Quick Thought: How about stopping before garbage trees are trained?\n",
    "What if we stop training more trees when the weighted errors $\\epsilon$'s stop improving? Then a lot of errorneous trees can be removed while retaining well-performing trees only.\n",
    "\n",
    "Let's study the above figure again. For example, if we stop training at the <span style=\"color:green\"><u>green vertical line</u></span>, then <b>>150</b> trees on the right with high errors can be ignored, while retaining <b>~70</b> trees on the left with better performances. \n",
    "\n",
    "This can be achieved by limiting the weighted errors. In the figure, the upper/lower bounds are given by the <span style=\"color:gray\"><u>grey horizontal lines</u></span>. If the <span style=\"color:orange\">error</span> (orange) exceeds the boundaries too much, then there is a high possibility that it will not be improving too much later on, so one can stop the training, and training more trees is not helpful anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Contribution: Early Stopping\n",
    "In this project, we introduced the use of early stopping for BRF, which is going to be explained in this section. \n",
    "\n",
    "## Algorithm\n",
    "The following gives the modified version of BRF algorithm (with <b><u>modifications bolded and underlined</u></b>):\n",
    "- for 1:T do\n",
    "    - train a tree\n",
    "    - predict with the forest\n",
    "    - evaluate weighted error $\\epsilon$ and tree weights $\\alpha$\n",
    "        - <b><u>If $\\epsilon$ exceeds boundaries for too many times</u></b>\n",
    "            - <b><u>Stop training</u></b>\n",
    "        - If $\\alpha$ > 0 \n",
    "            - reject tree\n",
    "\n",
    "## Try Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a BRF classifier with Early Stopping \"\"\"\n",
    "# Set paramters to enable Early Stopping\n",
    "# Can be easily done by specifying error boundaries and setting 'early_stop' as True\n",
    "brf_es_params = {'T': 250,\n",
    "                 'depth_max': 20,\n",
    "                 'eps_ub': 0.5,\n",
    "                 'eps_lb': 0,\n",
    "                 'early_stop': True}\n",
    "brf_es_clf = BoostedRandomForest(**brf_es_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  111.31020283699036\n",
      "Acc:  0.7341442224152911\n",
      "# Trees:  255\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sample Procedures of Experiment \"\"\"\n",
    "# Train BRF classifier\n",
    "start = time()\n",
    "brf_es_clf.fit(X_train, y_train)\n",
    "end = time()\n",
    "print(\"Training time: \", end-start)\n",
    "\n",
    "# Give predictions\n",
    "pred = brf_es_clf.ensemble_predict(X_test)\n",
    "\n",
    "# Calcuate Accuracy\n",
    "print(\"Acc: \", accuracy_score(y_test, pred))\n",
    "\n",
    "# Get # trees used in classifier\n",
    "T = len(brf_es_clf.clfs)\n",
    "print(\"# Trees: \", T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           \n",
    "## Improvements\n",
    "In short, this proposed early stopping algorithm improves BRF with:\n",
    "\n",
    "1. Retain the nearly optimal set of trees in forest in automated manner\n",
    "<img src=\"Plots/early_stop.png\" width=\"400\"/>\n",
    "2. Reduce forest size and hence memory usage\n",
    "2. Greatly reduce training time\n",
    "<img src=\"Pics/table.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "## Justifacation\n",
    "Here gives some simple justifications on why the proposed method works.\n",
    "\n",
    "1. Safely stop after but near a global optimal.\n",
    "    - Given BRF as a <b>boosting</b> method\n",
    "        - Possibily attains its optimal at early stage\n",
    "    - Meanwhile, <b>random sampling</b> in BRF assures comparable performance for testing data\n",
    "<br><br>\n",
    "2. Early Stopping can be done on the fly.\n",
    "    - Similar to 1., performance of BRF tends to drop in one way\n",
    "<br><br>\n",
    "3. Introduces lower bounds and upper bound for weighted error.\n",
    "    - Adding more trees may not be helpful, since\n",
    "        - \"weighted error $\\epsilon$ --> 0\"         ==> Overfitting (improved too much for training set)\n",
    "        - \"weighted error $\\epsilon$ --> 1/n_class\" ==> Random Guess (hard to improve more)\n",
    "\n",
    "## Significance\n",
    "Given with the properties of BRF (like 1. in previous subsection), the early stopping algorithm proposed here is able to achieve more than the early stopping mechanisms in common place.\n",
    "\n",
    "The most common practice of early stopping is the validation-based early stopping. The following gives an algorithm excerpted from <a href=\"https://en.wikipedia.org/wiki/Early_stopping\">Wikipedia</a>:\n",
    "```\n",
    "1. Split the training data into a training set and a validation set, e.g. in a 2-to-1 proportion.\n",
    "2. Train only on the training set and evaluate the per-example error on the validation set once in a while, e.g. after every fifth epoch.\n",
    "3. Stop training as soon as the error on the validation set is higher than it was the last time it was checked.\n",
    "4. Use the weights the network had in that previous step as the result of the training run.\n",
    "```\n",
    "But, some problems are spotted from the algorithm above:\n",
    "1. How could we ensure the the error is not stuck at local minima?\n",
    "2. Since validation errors usually fluctuate a lot, how could we distinguish fluctating errors from rising errors?\n",
    "\n",
    "Usually, the solution is: __TRAIN MORE__ and you will see.\n",
    "<br><br>\n",
    "However, with our proposed algorithm, you can stop expanding BRF immediately when the criteria are met.\n",
    "<br><br>\n",
    "This discussion is going to be summarized by a comparison between two figures, with the left showing the steps required for picking the best place to stop, and the right showing that with the proposed method.\n",
    "<br><br>\n",
    "<img src=\"Pics/es_comp.png\" width=\"1000\"/>\n",
    "   <div align=\"center\">...............................Validation-based Early Stopping ...................................................................................      BRF with Early Stopping...............................  </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "---\n",
    "This notebook presented our studies on the algorithm \"Boosted Random Forest\" [1], which applies Boosting to the bagging algorithm Random Forest, and takes advantages from both methods to give a RF classifier with fewer trees to give better performance. \n",
    "\n",
    "For the purpose of the project, BRF is implemented as a python module, which is also used for experiments presented in this notebook. The results do show that BRF does improves on RF in the way as it is designed.\n",
    "\n",
    "Moreover, this project proposes an improvement to BRF with the use of early stopping. With the proposed improvement, training of BRF classifier can be stopped once its optimal performance is achieved, without sacrifices in accuracy.\n",
    "\n",
    "For more information about how to use the python module, please refer to example.ipynb for more use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "---\n",
    "[1] Mishina, Y., Murata, R., Yamauchi, Y., Yamashita, T., & Fujiyoshi, H. (2015). Boosted random forest. IEICE TRANSACTIONS on Information and Systems, 98(9), 1630-1636."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
